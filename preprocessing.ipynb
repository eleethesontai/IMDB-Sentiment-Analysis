{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import random\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['x'] = samples['Review'].str.lower()\n",
    "samples['x'] = samples['x'].str.replace(r'[^a-z\\s\\']', ' ', regex=True)\n",
    "samples['x'] = samples['x'].str.replace(r'\\'', '', regex=True)\n",
    "samples['x'] = samples['x'].str.replace(r'\\s+', ' ', regex=True)\n",
    "samples['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(samples['x'], vocab_size=1022, min_frequency=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tokenizer.get_vocab()\n",
    "vocabulary = {i:v+2 for i,v in sorted(vocabulary.items(), key=lambda item: item[1])}\n",
    "vocabulary.update({\"[PAD]\": 0, \"[MASK]\": 1})\n",
    "vocabulary = [i for i,v in sorted(vocabulary.items(), key=lambda item: item[1])]\n",
    "vocabulary_size = len(vocabulary)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['x'] = samples['x'].apply(lambda x: tokenizer.encode(x).ids)\n",
    "sequence_size = samples['x'].apply(len).max()\n",
    "samples['x'] = samples['x'].apply(lambda x: (([0]*sequence_size)+x)[-sequence_size:])\n",
    "samples['x'] = samples['x'].apply(lambda x: np.array(x))\n",
    "\n",
    "sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [list(samples[samples['x'].apply(lambda x: t in x)].index) for t in range(vocabulary_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(samples['x'].tolist()).astype(np.uint16)\n",
    "y = np.array(samples['Sentiment'].tolist()).astype(np.uint8)\n",
    "v = np.array([s.encode('utf-8') for s in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('preprocessed.h5', 'w')\n",
    "\n",
    "file.create_dataset('x', data=x, maxshape=(None, x.shape[1]), chunks=True, compression='gzip')\n",
    "file.create_dataset('y', data=y, maxshape=(None,), chunks=True, compression='gzip') \n",
    "file.create_dataset('v', data=v)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()\n",
    "file = h5py.File('preprocessed.h5', 'a')\n",
    "sample_rate = .1\n",
    "_indices = [random.sample(i,int(len(i)*sample_rate)) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "for t in range(vocabulary_size):\n",
    "    i = _indices[t]\n",
    "    c += len(i) \n",
    "\n",
    "    if len(i)>0:\n",
    "        _x = np.array([np.where(a == t, 1, a) for a in np.array(samples.loc[i]['x'])])\n",
    "        _y = np.array(samples.loc[i]['Sentiment'])\n",
    "\n",
    "        file['x'].resize(file['x'].shape[0] + _x.shape[0], axis=0)\n",
    "        file['x'][-_x.shape[0]:] = _x\n",
    "\n",
    "        file['y'].resize(file['y'].shape[0] + _y.shape[0], axis=0)\n",
    "        file['y'][-_y.shape[0]:] = _y\n",
    "\n",
    "    file.flush()\n",
    "\n",
    "file.close()\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
