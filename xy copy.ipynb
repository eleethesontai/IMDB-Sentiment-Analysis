{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        working with one of the best shakespeare sourc...\n",
       "1        well tremors i the original started off in and...\n",
       "2        ouch this one was a bit painful to sit through...\n",
       "3        ive seen some crappy movies in my life but thi...\n",
       "4         carriers follows the exploits of two guys and...\n",
       "                               ...                        \n",
       "49995    this movie is certainly well constructed begin...\n",
       "49996    nice to see a comedy for grown ups masterfully...\n",
       "49997    jean renoirs homage to the paris of the late t...\n",
       "49998    what are the movies i mean what are movies mad...\n",
       "49999    i saw this movie on tv and loved it i am a rea...\n",
       "Name: x, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['x'] = samples['Review'].str.lower()\n",
    "samples['x'] = samples['x'].str.replace(r'[^a-z\\s\\']', ' ', regex=True)\n",
    "samples['x'] = samples['x'].str.replace(r'\\'', '', regex=True)\n",
    "samples['x'] = samples['x'].str.replace(r'\\s+', ' ', regex=True)\n",
    "samples['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(samples['x'], vocab_size=1024, min_frequency=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = tokenizer.get_vocab()\n",
    "vocubulary_size = len(vocabulary)\n",
    "vocubulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['x'] = samples['x'].apply(lambda x: tokenizer.encode(x).ids)\n",
    "sequence_size = samples['x'].apply(len).max()\n",
    "samples['x'] = samples['x'].apply(lambda x: (([0]*sequence_size)+x)[-sequence_size:])\n",
    "\n",
    "sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                               ...                        \n",
       "49995    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "49996    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "49997    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "49998    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "49999    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: x, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m pair_freq_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     21\u001b[0m     [(i, j, co_occurrence_matrix[i, j]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vocab_size) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, vocab_size)],\n\u001b[1;32m     22\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken Pair Frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mpair_freq_df)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Assuming 'vocab_size' is 1024 and 'df' is your DataFrame with a column 'x' containing token lists\n",
    "vocab_size = 1024\n",
    "\n",
    "# Step 1: Initialize an empty co-occurrence matrix\n",
    "co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=int)\n",
    "\n",
    "# Step 2: Iterate over each sample and update the co-occurrence matrix\n",
    "for tokens in samples['x']:\n",
    "    # For each sample, get unique token combinations\n",
    "    unique_pairs = combinations(set(tokens), 2)  # Set removes duplicates within a sample\n",
    "    for i, j in unique_pairs:\n",
    "        co_occurrence_matrix[i, j] += 1\n",
    "        co_occurrence_matrix[j, i] += 1  # Ensure symmetry\n",
    "\n",
    "# Step 3: Convert matrix to DataFrame\n",
    "pair_freq_df = pd.DataFrame(\n",
    "    [(i, j, co_occurrence_matrix[i, j]) for i in range(vocab_size) for j in range(i + 1, vocab_size)],\n",
    "    columns=['Token1', 'Token2', 'Frequency']\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Token Pair Frequency\", dataframe=pair_freq_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
